version: '3.8'

services:
  # Frontend Service - Nginx serving React/Vue app
  frontend:
    build: ./Frontend
    container_name: legal-ai-frontend
    ports:
      - "5173:80"
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - legal-ai-network
    restart: unless-stopped

  # Backend Service - FastAPI application
  backend:
    build: ./Backened
    container_name: legal-ai-backend
    ports:
      - "8000:8000"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    volumes:
      - ./Backened/document:/app/document
      - backend_cache:/app/.cache
    networks:
      - legal-ai-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Redis Service - Caching layer
  redis:
    image: redis:alpine
    container_name: legal-ai-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - legal-ai-network
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5

  # Ollama Service - AI Model Server
  ollama:
    image: ollama/ollama:latest
    container_name: legal-ai-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - legal-ai-network
    restart: unless-stopped
    # Uncomment below for GPU support (NVIDIA)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:11434/api/tags" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

networks:
  legal-ai-network:
    driver: bridge

volumes:
  redis_data:
  ollama_data:
  backend_cache:
